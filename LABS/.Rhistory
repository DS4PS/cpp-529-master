rm(list = ls())
library(sf)
library(tidyverse)
library(tigris)
library(tidycensus)
library(ggrepel)
options(tigris_use_cache=TRUE)
options(tigris_class="sf")
census_api_key("Your Key here")
acs <- get_acs("tract", table = "B15003", cache_table = TRUE,
geometry = TRUE, state = "AZ", county = "Maricopa County",
year = 2017, output = "tidy")
census_api_key("8eab9b16f44cb26460ecbde164482194b7052772")
acs <- get_acs("tract", table = "B15003", cache_table = TRUE,
geometry = TRUE, state = "AZ", county = "Maricopa County",
year = 2017, output = "tidy")
acs12 <- get_acs("tract", table = "B15003", cache_table = TRUE,
geometry = FALSE, state = "AZ", county = "Maricopa County",
year = 2012, output = "tidy")
acs <- merge(acs,acs12, by.all="GEOID", all.x=TRUE) # all.x=TRUE makes sure that the merged dataframe keeps all counties in CenDF even if missing in CenDF2012
acs <- acs %>%
mutate( pct_change = 100 * (`estimate` - `estimate2012`) / `estimate2012`,
change = estimate - estimate2012,
abs.change = abs(change),
change.type = ifelse( change >= 0, "POS", "NEG" ) )
acs12 <- acs12 %>%
mutate(
id = str_extract(variable, "[0-9]{3}$") %>% as.integer
) %>%
# variable 1 is the "total", which is just the sum of the others
filter(id > 1) %>%
mutate(education =case_when(
id %>% between(2, 16) ~ "No HS diploma",
id %>% between(17, 21) ~ "HS, no Bachelors",
id > 21 ~ "At Least a Bachelors"
)) %>%
group_by(GEOID, education) %>%
summarise(estimate2012 = sum(estimate))
acs <- merge(acs,acs12, by.all="GEOID", all.x=TRUE) # all.x=TRUE makes sure that the merged dataframe keeps all counties in CenDF even if missing in CenDF2012
acs <- acs %>%
mutate( pct_change = 100 * (`estimate` - `estimate2012`) / `estimate2012`,
change = estimate - estimate2012,
abs.change = abs(change),
change.type = ifelse( change >= 0, "POS", "NEG" ) )
acs_split <- acs %>%
filter(estimate > 50) %>%
split(.$education)
generate_samples <- function(data)
suppressMessages(st_sample(data, size = round(data$estimate / 100)))
points <- map(acs_split, generate_samples)
points <- map(acs_split, generate_samples)
(See Appendix Figure \ref{fig:figSLCPCoverage})
(See Appendix Figure \ref{fig:figEthnicPMaps})
## Program Features
SLCP is ran by the State Forestry Administration (SFA).   The SFA determines the size of the retirement quotas and distributes them to provincial governments who are responsible for project coordination and planning.  Provincial governemnts then allocate the retirement quotas  to the local governments at the county and township level who are responsible for project implementation. Local governments sign liability agreements are held responsible for meeting the targets set by the SFA.   From there, quotas are allocated to participating households who are asked to submit an initial application expressing interest in  participating in the program.   Despite the bottom-up application process, case study evidences suggests that pariticipation in the program is more quasi-voluntary [@uchida2009conservation].
install.packages("splitstackshape")
rm(list=ls())
gc()
require("rvest")
require("xml2")
require("readr")
require("stringr")
require("ggplot2")
require("ggthemes")
require("scales")
require("viridis")
require("raster")
require("sp")
require("splitstackshape")
require("plyr")
require("dplyr")
options(scipen=999)
# Creating a dataset by scraping data from the "I Paid a Bribe"" website, linking to pre-scraped dataset below this block
# General considerations about the layout of the website:
# Data about the bribe; time, place, amount, category, etc. is available from the main search page.
# The full text describing the event is only available from the link itself.
# Can get most of the data from the main "search", without having to go through every link that has been pulled
## Initialize dataframe with the desired data columns
df <- data_frame(title = character(),
amount = character(),
dep = character(),
trans = character(),
views = character(),
city = character(),
date = character(),
time = character())
## initialize empty dataframe for
dftemp <- data_frame()
## Scrape data from "I paid a bribe" website
setwd("~/Dropbox/TeachingResources/R/Rexcercises/CorruptionIndia")
#35100
for (i in 115:14659) { #loop through first 100 pages, 10 results per page = 1000
link <- paste("http://www.ipaidabribe.com/reports/paid?page=",i*10, sep = "") # Create hyperlink based on loop function
print(paste("processing", i, sep = " ")) # progress report
main <- read_html(link, encoding = "UTF-8") # define the static part of link references
title <- main %>%
html_nodes(".heading-3 a") %>%
html_text()
amount <- main %>% # feed `main.page` to the next step
html_nodes(".paid-amount span") %>% # get the CSS nodes
html_text() # extract the link text
dep <- main %>% # feed `main.page` to the next step
html_nodes(".name a") %>% # get the CSS nodes
html_text() # extract the link text
trans <- main %>% # feed `main.page` to the next step
html_nodes(".transaction a") %>% # get the CSS nodes
html_text() # extract the link text
views <- main %>% # feed `main.page` to the next step
html_nodes(".overview .views") %>% # get the CSS nodes
html_text() # extract the link text
city <- main %>% # feed `main.page` to the next step
html_nodes(".location") %>% # get the CSS nodes
html_text() # extract the link text
date <- main %>% # feed `main.page` to the next step
html_nodes(".date") %>% # get the CSS nodes
html_text() # extract the link text
time <- main %>% # feed `main.page` to the next step
html_nodes(".time-span") %>% # get the CSS nodes
html_text() # extract the link text
dftemp <- cbind(title, amount, dep, trans, views, city, date, time) # bind the variables together into a 10 by n dataframe
df <- rbind(df,dftemp) # rbind the temp dataframe for this page to the main dataframe
Sys.sleep(1) # timer, wait 1 second
cat(" done!\n") #progress report
}
#35100
for (i in 1:120) { #loop through first 100 pages, 10 results per page = 1000
link <- paste("http://www.ipaidabribe.com/reports/paid?page=",i*10, sep = "") # Create hyperlink based on loop function
print(paste("processing", i, sep = " ")) # progress report
main <- read_html(link, encoding = "UTF-8") # define the static part of link references
title <- main %>%
html_nodes(".heading-3 a") %>%
html_text()
amount <- main %>% # feed `main.page` to the next step
html_nodes(".paid-amount span") %>% # get the CSS nodes
html_text() # extract the link text
dep <- main %>% # feed `main.page` to the next step
html_nodes(".name a") %>% # get the CSS nodes
html_text() # extract the link text
trans <- main %>% # feed `main.page` to the next step
html_nodes(".transaction a") %>% # get the CSS nodes
html_text() # extract the link text
views <- main %>% # feed `main.page` to the next step
html_nodes(".overview .views") %>% # get the CSS nodes
html_text() # extract the link text
city <- main %>% # feed `main.page` to the next step
html_nodes(".location") %>% # get the CSS nodes
html_text() # extract the link text
date <- main %>% # feed `main.page` to the next step
html_nodes(".date") %>% # get the CSS nodes
html_text() # extract the link text
time <- main %>% # feed `main.page` to the next step
html_nodes(".time-span") %>% # get the CSS nodes
html_text() # extract the link text
dftemp <- cbind(title, amount, dep, trans, views, city, date, time) # bind the variables together into a 10 by n dataframe
df <- rbind(df,dftemp) # rbind the temp dataframe for this page to the main dataframe
Sys.sleep(1) # timer, wait 1 second
cat(" done!\n") #progress report
}
#35100
for (i in 1:15) { #loop through first 100 pages, 10 results per page = 1000
link <- paste("http://www.ipaidabribe.com/reports/paid?page=",i*10, sep = "") # Create hyperlink based on loop function
print(paste("processing", i, sep = " ")) # progress report
main <- read_html(link, encoding = "UTF-8") # define the static part of link references
title <- main %>%
html_nodes(".heading-3 a") %>%
html_text()
amount <- main %>% # feed `main.page` to the next step
html_nodes(".paid-amount span") %>% # get the CSS nodes
html_text() # extract the link text
dep <- main %>% # feed `main.page` to the next step
html_nodes(".name a") %>% # get the CSS nodes
html_text() # extract the link text
trans <- main %>% # feed `main.page` to the next step
html_nodes(".transaction a") %>% # get the CSS nodes
html_text() # extract the link text
views <- main %>% # feed `main.page` to the next step
html_nodes(".overview .views") %>% # get the CSS nodes
html_text() # extract the link text
city <- main %>% # feed `main.page` to the next step
html_nodes(".location") %>% # get the CSS nodes
html_text() # extract the link text
date <- main %>% # feed `main.page` to the next step
html_nodes(".date") %>% # get the CSS nodes
html_text() # extract the link text
time <- main %>% # feed `main.page` to the next step
html_nodes(".time-span") %>% # get the CSS nodes
html_text() # extract the link text
dftemp <- cbind(title, amount, dep, trans, views, city, date, time) # bind the variables together into a 10 by n dataframe
df <- rbind(df,dftemp) # rbind the temp dataframe for this page to the main dataframe
Sys.sleep(1) # timer, wait 1 second
cat(" done!\n") #progress report
}
#clean unused variables from workspace
rm("title", "amount", "dep", "trans", "views", "city", "date", "time", "dftemp", "i")
save.image("~/Dropbox/TeachingResources/R/Rexcercises/CorruptionIndia/2015Data.RData")
df$states <- lapply(strsplit(as.character(df$city), "\\,"), "[", 2)
df$city
df$states <- lapply(strsplit(as.character(df$city), "\\,"), "[", 2)
df$city <- lapply(strsplit(as.character(df$city), "\\,"), "[", 1)
## Clean and order dataset, force correct data types, factors for faceting and checking number of levels
clean <- function(df) {
df$title <- df$title %>% # clean text
str_replace_all(pattern = "\\n" , replacement = " ") %>%
str_trim()
df$dep <- df$dep %>%
as.factor() # convert to factor
df$trans <- df$trans %>%
as.factor() # convert to factor
df$amount <- df$amount %>% # clean text from amount and convert to numeric
str_replace_all(pattern = "Paid INR" , replacement = " ") %>%
str_replace_all(pattern = "," , replacement = "") %>%
str_trim() %>%
as.numeric()
df$views <- df$views %>% # clean text from views and convert to numeric
str_replace_all(pattern = "views" , replacement = " ") %>%
str_trim() %>%
as.numeric()
df$city <- df$city %>% # clean text from city
as.character() %>%
str_trim() %>%
as.factor() # conver tot factor
df$states <- df$states %>% # clean text from states
as.character(df$states) %>%
str_trim() %>%
as.factor() # convert to factor
df$time <- as.numeric(str_extract(df$time,"[0-9]*"))*!grepl("minutes|hours",df$time) # clean hours and minutes out of time stamp and change to whole number of days
df$date <- as.Date(df$date, format("%B %d, %Y")) # convert the date column to date format
df <- df[, c(1,2,3,4,5,6,9,7,8)] # rearrange columns
return (df)
}
### sorted in descending value
df %>%
arrange(desc(amount)) %>%
head(5)
### sorted in ascending order
df %>%
arrange(desc(desc(amount))) %>%
head(5)
## look for missing values in the character variables (title, dep, trans, city, state)
dfmissing <- df %>% filter(title == "" | dep == "" | trans == "" | city == "" | states == "") %>% head(5)
dfmissing
## look for missing values in the character variables (title, dep, trans, city, state)
dfmissing <- df %>% filter(title == "" | dep == "" | trans == "" | city == "" | states == "") %>% head(5)
## count number of rows with NA values
narows <- nrow(df[!complete.cases(df),])
## test for misassigned values in either
duplicate <- match(df$city,df$states)
duplicate2 <- match(df$states,df$city)
dupcheck <- df[!is.na(duplicate),]
dupcheck2 <- df[!is.na(duplicate2),]
## remove unwanted variables again used
rm("dfmissing", "narows", "duplicate", "duplicate2", "dupcheck", "dupcheck2")
summary(df)
library(sf)
library(tidyverse)
library(tigris)
library(tidycensus)
library(ggrepel)
options(tigris_use_cache=TRUE)
options(tigris_class="sf")
census_api_key("8eab9b16f44cb26460ecbde164482194b7052772")
### Bring in 2017 variable related to educational attainment
acs <- get_acs("tract", table = "B15003", cache_table = TRUE,
geometry = TRUE, state = "AZ", county = "Maricopa County",
year = 2017, output = "tidy")
acs <- acs %>%
mutate(
id = str_extract(variable, "[0-9]{3}$") %>% as.integer
) %>%
# variable 1 is the "total", which is just the sum of the others
filter(id > 1) %>%
mutate(education =case_when(
id %>% between(2, 16) ~ "No HS diploma",
id %>% between(17, 21) ~ "HS, no Bachelors",
id > 21 ~ "At Least a Bachelors"
)) %>%
group_by(GEOID, education) %>%
summarise(estimate = sum(estimate))
acs12 <- get_acs("tract", table = "B15003", cache_table = TRUE,
geometry = FALSE, state = "AZ", county = "Maricopa County",
year = 2012, output = "tidy")
acs12 <- acs12 %>%
mutate(
id = str_extract(variable, "[0-9]{3}$") %>% as.integer
) %>%
# variable 1 is the "total", which is just the sum of the others
filter(id > 1) %>%
mutate(education =case_when(
id %>% between(2, 16) ~ "No HS diploma",
id %>% between(17, 21) ~ "HS, no Bachelors",
id > 21 ~ "At Least a Bachelors"
)) %>%
group_by(GEOID, education) %>%
summarise(estimate2012 = sum(estimate))
acs <- merge(acs,acs12, by.all="GEOID", all.x=TRUE) # all.x=TRUE makes sure that the merged dataframe keeps all counties in CenDF even if missing in CenDF2012
acs <- acs %>%
mutate( pct_change = 100 * (`estimate` - `estimate2012`) / `estimate2012`,
change = estimate - estimate2012,
abs.change = abs(change),
change.type = ifelse( change >= 0, "POS", "NEG" ) )
acs_split <- acs %>%
filter(estimate > 50) %>%
split(.$education)
generate_samples <- function(data)
suppressMessages(st_sample(data, size = round(data$estimate / 100)))
points <- map(acs_split, generate_samples)
points
points <- imap(points,
~st_sf(data_frame(education = rep(.y, length(.x))),
geometry = .x))
points <- do.call(rbind, points)
points <- points %>% group_by(education) %>% summarise()
points <- points %>%
mutate(education = factor(
education,
levels = c("No HS diploma", "HS, no Bachelors",
"At Least a Bachelors")))
# view how many points are in each layer
points %>% mutate(n_points = map_int(geometry, nrow))
theme_set( theme_minimal() +
theme(panel.grid.major = element_line(size = 0),
plot.background = element_rect( fill = "#fdfdfd",
colour = NA ),
axis.title = element_blank(),
axis.text = element_blank(),
legend.position = "bottom"))
plot7<-ggplot() +
geom_sf(data = points,
aes(colour = education,
fill = education),
size = .1) +
scale_color_brewer( type = 'qual', palette = 2 ) +
scale_fill_brewer( type = 'qual', palette = 2 )+
ggtitle("Distribution of educational attainment in Maricopa County",
"1 dot equals 100 people")
plot7
acs_bachelors <-
acs %>%
filter( education == 'At Least a Bachelors' )
acs.split.bachelors <- acs %>%
# filter( estimate > 50) %>%
split( .$change.type )
generate_samples <- function(data)
{
suppressMessages(
st_sample( data, size = round(data$abs.change / 100 ) )
)
}
pointsChange <- map( acs.split.bachelors, generate_samples )
pointsChange <- imap( pointsChange,
~st_sf(data_frame( change.type = rep(.y, length(.x))),
geometry = .x))
pointsChange <- do.call( rbind, pointsChange )
URL <- "https://github.com/DS4PS/cpp-529-master/raw/master/data/CensusData.rds"
census.dats <- readRDS(gzcon(url( URL )))
census.dats <- readRDS(gzcon(url( URL )))
census.dats<-na.omit(census.dats)
library(plyr)
censusChange0010<-ddply(census.dats,"TRTID10",summarise,
HousePrice = Median.HH.Value00/(Median.HH.Value10+1),
Foreign_born = Foreign.Born00/(Foreign.Born10+1),
Recent_immigrant = Recent.Immigrant00 /(Recent.Immigrant10+1),
Poor_english = Poor.English00/(Poor.English10+1),
Veteran = Veteran00/(Veteran10+1),
Poverty = Poverty00/(Poverty10+1),
Poverty_black = Poverty.Black00 /(Poverty.Black10+1),
Poverty_white = Poverty.White00 /(Poverty.White10+1),
Poverty_hispanic = Poverty.Hispanic00 /(Poverty.Hispanic10+1),
pop_black = Pop.Black00/(Pop.Black10+1),
pop_hispanic = Pop.Hispanic00 /(Pop.Hispanic10+1),
pop_unemp = Pop.Unemp00 /(Pop.Unemp10+1),
pop_manufact = Pop.Manufact00 /(Pop.Manufact10+1),
pop_selfemp = Pop.SelfEmp00 /(Pop.SelfEmp10+1),
pop_prof = Pop.Prof00/(Pop.Prof10+1),
Female_laborforce = Female.LaborForce00/ (Female.LaborForce10+1)
)
censusChange0010<-ddply(census.dats,"TRTID10",summarise,
HousePrice = Median.HH.Value00/(Median.HH.Value10+1),
Foreign_born = Foreign.Born00/(Foreign.Born10+1),
Recent_immigrant = Recent.Immigrant00 /(Recent.Immigrant10+1),
Poor_english = Poor.English00/(Poor.English10+1),
Veteran = Veteran00/(Veteran10+1),
Poverty = Poverty00/(Poverty10+1),
Poverty_black = Poverty.Black00 /(Poverty.Black10+1),
Poverty_white = Poverty.White00 /(Poverty.White10+1),
Poverty_hispanic = Poverty.Hispanic00 /(Poverty.Hispanic10+1),
pop_black = Pop.Black00/(Pop.Black10+1),
pop_hispanic = Pop.Hispanic00 /(Pop.Hispanic10+1),
pop_unemp = Pop.Unemp00 /(Pop.Unemp10+1),
pop_manufact = Pop.Manufact00 /(Pop.Manufact10+1),
pop_selfemp = Pop.SelfEmp00 /(Pop.SelfEmp10+1),
pop_prof = Pop.Prof00/(Pop.Prof10+1),
Female_laborforce = Female.LaborForce00/ (Female.LaborForce10+1)
)
